{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DEFINITION] Métodos para tratamento dos dados com o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def aggregate_tags_count(new_values, total_sum):\n",
    "    return sum(new_values) + (total_sum or 0)\n",
    "\n",
    "\n",
    "def get_sql_context_instance(spark_context):\n",
    "    \n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "        \n",
    "    return globals()['sqlContextSingletonInstance']\n",
    "\n",
    "def get_hashtags(words):\n",
    "    return set(w[1:] for w in words.split() if w.startswith('#'))\n",
    "\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    \n",
    "    try:\n",
    "        # Pegando o contexto corrente do spark sql\n",
    "        #sql_context = get_sql_context_instance(rdd.context)\n",
    "        \n",
    "        # Convertendo RDD para Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        \n",
    "        print(\"================================\")\n",
    "        print(\"row_rdd: \")\n",
    "        print(row_rdd.take(1))\n",
    "        \n",
    "        # Criando um dataframe com a lista mapeada de Row RDD\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        \n",
    "        print(\"Convertendo para JSON: \")\n",
    "        data = hashtags_df.toJSON().collect()\n",
    "        print(data)\n",
    "        \n",
    "        for line in data:\n",
    "            result = requests.post(url = \"http://localhost:5001/v1/twits\", json = json.loads(line))\n",
    "            print(result)\n",
    "        \n",
    "        \n",
    "    except Exception as ex:\n",
    "        if rdd.isEmpty():\n",
    "            pass\n",
    "        else:\n",
    "            print(\"Error: %s\" % ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DEFINITION] Contexto do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataStream: \n",
      "<pyspark.streaming.dstream.DStream object at 0x0B003510>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "\n",
    "# Criando a configuração (Informe um nome para a sua API)\n",
    "conf = SparkConf(\"local[*]\")\n",
    "conf.setAppName(\"twitterAppForDev\")\n",
    "\n",
    "# Cria o contexto spark com a configuração acima\n",
    "sc = SparkContext(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\") #Para evitar muito lixo no log\n",
    "\n",
    "# Cria o contexto do straming para o contexto do spark acima utilizando como intervalo 3 segundos\n",
    "ssc = StreamingContext(sc, 3)\n",
    "\n",
    "# Setando o checkpoint para permitir a recuperação do RDD\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "\n",
    "# Lendo dados da porta 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\", 9009)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TRANSFORMATION] Tratamento dos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataStream: \n",
      "<pyspark.streaming.dstream.DStream object at 0x0B003510>\n",
      "================================\n",
      "================================\n",
      "words: \n",
      "<pyspark.streaming.dstream.TransformedDStream object at 0x0B003CD0>\n",
      "================================\n",
      "Processar tags_totals: \n",
      "<pyspark.streaming.dstream.DStream object at 0x0B003BD0>\n",
      "----------- 2020-06-04 17:24:54 -----------\n",
      "================================\n",
      "row_rdd: \n",
      "[]\n",
      "----------- 2020-06-04 17:24:57 -----------\n",
      "================================\n",
      "row_rdd: \n",
      "[]\n",
      "----------- 2020-06-04 17:25:00 -----------\n",
      "================================\n",
      "row_rdd: \n",
      "[]\n",
      "----------- 2020-06-04 17:25:03 -----------\n",
      "================================\n",
      "row_rdd: \n",
      "----------- 2020-06-04 17:25:06 -----------\n",
      "================================\n",
      "row_rdd: \n",
      "[]\n",
      "----------- 2020-06-04 17:25:09 -----------\n",
      "================================\n",
      "row_rdd: \n"
     ]
    }
   ],
   "source": [
    "# Quebrando cada tweet em palavras\n",
    "print(\"dataStream: \")\n",
    "print(dataStream)\n",
    "\n",
    "print(\"================================\")\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "print(\"================================\")\n",
    "print(\"words: \")\n",
    "print(words)\n",
    "\n",
    "# Filtra as palavras para obter apenas as hashtags, então mapeia cada hashtag para ser par com 1: (hashtag, 1)\n",
    "hashtags = words.filter(get_hashtags).map(lambda x: (x, 1))\n",
    "\n",
    "# Agregando as quantidades por hashtag\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "\n",
    "print(\"================================\")\n",
    "print(\"Processar tags_totals: \")\n",
    "print(tags_totals)\n",
    "\n",
    "# Processando cada os totais de cada tag com RDD\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "\n",
    "# Startando a computação do streaming\n",
    "ssc.start()\n",
    "\n",
    "# Esperando a finalização do straming\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
